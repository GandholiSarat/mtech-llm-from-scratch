# LLM From Scratch (CPU-First)

This project implements a GPT-style Transformer language model
from first principles using PyTorch.

The focus is on:
- Explicit tensor operations
- Educational clarity
- CPU-first development

This project is inspired by
Sebastian Raschkaâ€™s "LLMs-from-scratch" repository.

## Goals
- Build a decoder-only Transformer from scratch
- Implement tokenizer, attention, training, and inference manually
- Maintain a clean, well-documented codebase

## Current Status
- Project structure initialized
- CPU-only environment verified

